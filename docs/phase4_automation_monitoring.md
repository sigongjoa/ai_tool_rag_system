# Phase 4: 파이프라인 자동화, 평가 및 모니터링

## 목표
전체 시스템을 자동화하고, 성능을 지속적으로 측정하고 개선할 수 있는 체계를 마련합니다.

## 4-1. MLOps 파이프라인 구축

### 목표
데이터 수집, 전처리, 임베딩, 학습 데이터 생성, 모델 재학습 등 전체 워크플로우를 자동화하는 파이프라인을 구축합니다.

### 구현 상세
- **도구 선택:** Airflow, Prefect와 같은 워크플로우 오케스트레이션 도구를 사용하거나, 간단한 경우 Cron-job 스케줄러를 활용할 수 있습니다. 여기서는 개념 설명을 위해 가상의 스케줄러를 가정하고, 각 태스크를 실행하는 쉘 스크립트 형태로 구성할 수 있습니다.
- **파이프라인 순서 (DAG 예시):**
  1.  **URL 수집 (정기적):** 새로운 AI 도구 URL을 크롤링하여 `raw_html_storage`에 저장합니다.
  2.  **LLM 기반 전처리 및 인덱싱:** `backend/api_gateway/main.py`의 `/api/v1/process_url` 엔드포인트를 호출하여 수집된 HTML을 파싱하고, LLM으로 메타데이터를 추출하며, FAISS 인덱스를 업데이트합니다.
      - 각 URL에 대해 병렬적으로 호출하거나, URL 리스트를 받아 일괄 처리하는 별도의 백엔드 유틸리티를 개발할 수 있습니다.
  3.  **학습 데이터 자동 생성:** `scripts/generate_finetuning_data.py` 스크립트를 실행하여 `/api/v1/search` 로그를 기반으로 학습 데이터셋을 생성합니다.
  4.  **Synthetic QA 생성:** `scripts/generate_synthetic_data.py` 스크립트를 실행하여 추가적인 학습 데이터를 보강합니다.
  5.  **학습 데이터 병합 (선택 사항):** 생성된 실제 데이터와 Synthetic 데이터를 병합하는 스크립트 (`scripts/combine_datasets.py`)를 추가할 수 있습니다.
  6.  **모델 재학습:** `scripts/finetune_model.py` 스크립트를 실행하여 LM Studio 모델을 파인튜닝하고, 업데이트된 LoRA 어댑터를 `models/finetuned_adapter`에 저장합니다.
  7.  **LM Studio 모델 업데이트 (수동 또는 자동):** LM Studio 서버가 재학습된 LoRA 어댑터를 로드하도록 업데이트합니다. (대부분 수동 또는 LM Studio API를 통한 스크립팅 필요)

## 4-2. A/B 테스트 환경 구성

### 목표
새롭게 파인튜닝된 모델의 성능을 기존 모델과 비교 평가하여, 실제 서비스 환경에서의 개선 효과를 검증합니다.

### 구현 상세
- **프론트엔드:** 사용자의 일정 비율(예: 10% 또는 특정 그룹)에 대해 새로운 모델의 응답을 사용하고, 나머지 사용자에게는 기존 모델의 응답을 제공하도록 프론트엔드에서 제어 로직을 추가합니다.
- **백엔드 (`backend/api_gateway/main.py` 수정):**
  - `/api/v1/search` 엔드포인트 내에 A/B 테스트를 위한 로직을 추가합니다.
  - 사용자 세션 ID, 사용자 그룹(A/B) 등을 기반으로 LLM 호출 시 다른 API 엔드포인트(예: `LLM_API_BASE_NEW_MODEL`)를 사용하도록 분기합니다.
  - `LLM_API_BASE`와 `LLM_API_BASE_NEW_MODEL` 두 가지 환경 변수를 관리해야 합니다.
  - 사용자 그룹 정보를 저장하거나, 요청 헤더 등으로 전달받아 사용합니다.
- **데이터 수집:** A/B 테스트 기간 동안 각 그룹별로 사용자 만족도, 답변 품질, 응답 속도 등 주요 지표를 별도로 수집하고 저장합니다.

## 4-3. 모니터링 대시보드 구축

### 목표
시스템의 핵심 지표(성능, 비용, 오류 등)를 시각화하여 운영 상황을 한눈에 파악하고 문제 발생 시 신속하게 대응할 수 있도록 합니다.

### 구현 상세
- **도구 선택:** Grafana, ELK Stack (Elasticsearch, Logstash, Kibana), Prometheus/Grafana 스택, 또는 간단한 Streamlit 애플리케이션 등을 활용할 수 있습니다.
- **모니터링 대상 및 지표:**
  - **API Gateway:**
    - `/api/v1/search` 요청 수, 응답 시간(latency), 오류율
    - `/api/v1/process_url` 요청 수, 성공/실패율, 처리 시간
  - **LLM 호출:**
    - LLM API 호출 수, 평균 응답 시간
    - 토큰 사용량 (입력 토큰, 출력 토큰) - LLM 서비스의 응답에서 토큰 정보를 추출하여 기록
  - **FAISS 인덱스:**
    - 현재 인덱스에 저장된 벡터 수 (`faiss_index.ntotal`)
    - 인덱스 로드 시간
  - **데이터 처리 파이프라인:**
    - 주간/월간 수집된 URL 수
    - LLM 추출 성공률, 실패율
    - 청크 생성 수
  - **모델 학습:**
    - 학습 실행 횟수, 학습 시간
    - 평가 셋에서의 정확도, 손실(loss) 변화
  - **사용자 경험 (A/B 테스트 결과):**
    - 각 모델 그룹별 사용자 만족도 (설문 또는 간접 지표)
    - Top miss-queries: LLM이 제대로 답변하지 못한 질문 목록 (수동 검토 필요)

### 구현 방식 (예시: 로그 기반 모니터링)
- `backend/api_gateway/main.py`의 로깅 기능을 강화하여 모든 중요 이벤트와 지표를 구조화된 로그(예: JSON 형식)로 출력합니다.
- Logstash 또는 Fluentd와 같은 도구를 사용하여 이 로그를 Elasticsearch로 수집합니다.
- Kibana를 사용하여 Elasticsearch에 저장된 로그 데이터를 기반으로 대시보드를 구축하고 시각화합니다. 